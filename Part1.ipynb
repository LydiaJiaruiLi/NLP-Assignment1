{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Corpus processing (legal text): tokenization and word counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(dir):\n",
    "    files = os.listdir(dir)\n",
    "    docs = []\n",
    "    for file in files:\n",
    "        f = open(dir + file, 'r')\n",
    "        docs.append(f.read().lower())\n",
    "        f.close()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = read_files('CUAD_v1/full_contract_txt/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4792127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = []\n",
    "for doc in docs:\n",
    "    tokens += tknzr.tokenize(doc)\n",
    "print(len(tokens))\n",
    "\n",
    "# Save tokens to output.txt\n",
    "f = open('outputs/output.txt', 'w')\n",
    "SEP = '\\n'\n",
    "f.write(SEP.join(tokens))\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40100\n",
      "Question b: 0.008367891752451468\n"
     ]
    }
   ],
   "source": [
    "# Count types\n",
    "counts = Counter(tokens)\n",
    "\n",
    "print(len(counts))\n",
    "print('Question b:', len(counts)/len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 tokens with the most occurrences\n",
      "[('the', 257211), (',', 241445), ('of', 156123), ('.', 139317), ('and', 132850), ('to', 129884), ('or', 108938), ('in', 79954), (')', 76081), ('(', 75059), ('any', 62239), ('*', 61928), ('a', 53118), ('shall', 48794), ('\"', 44393), ('by', 44317), ('agreement', 43631), ('this', 39989), ('be', 39702), ('for', 38727)]\n"
     ]
    }
   ],
   "source": [
    "# Sort tokens by occurrence\n",
    "print('The top 20 tokens with the most occurrences:')\n",
    "print(counts.most_common(20))\n",
    "\n",
    "f = open('outputs/tokens.txt', 'w')\n",
    "for each in counts.most_common():\n",
    "    f.write(each[0] + '\\t' + str(each[1]) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15521 tokens only appeared once\n"
     ]
    }
   ],
   "source": [
    "# Print tokens appeared only once\n",
    "once = 0\n",
    "for each in counts.most_common():\n",
    "    if each[1] == 1:\n",
    "        once += 1\n",
    "print(once, 'tokens only appeared once')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
